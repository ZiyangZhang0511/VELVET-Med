itc_module:
    contrastive_bottleneck: "pooling" # choose from "pooling", "convolution", "self-attention"
    contrastive_hidden_dim: 512
    itc_temperature: 0.07
    learnable_temp: True
    alpha_i2t: 0.5

    alpha_report_itc: 1.0
    alpha_sentence_itc: 1.0
    alpha_word_itc: 1.0

    last_n_layers_for_itc: 1

    similarity_type: "t" # choose from "tmb", "tb", "tm", "t"


mm_module:
    requires_match_score: False

    alpha_mm_mlm: 1.0
    alpha_mm_match: 1.0


loss_weights:
    alpha_itc: 1.0
    alpha_mm_ssl: 1.0
    alpha_vis_ssl: 1.0
    alpha_txt_ssl: 1.0


enable_modules:
    report_itc: True
    sentence_itc: True
    word_itc: True

    mm_ssl: True
    mm_match: True
    mm_mlm: True

    vision_ssl: False
    vision_contras: False
    vision_rot: False
    vision_recon: False
    
    text_ssl: False


vision_ssl:
    vision_encoder:
        input_size: [1, 96, 96, 96] # [C, H, W, D]
        name: "swinvit"
        swinvit:
            in_chans: 1
            embed_dim: 48
            window_size: 7
            patch_size: 2
            depths: [2, 8, 8, 8]
            num_heads: [4, 8, 16, 32]
            mlp_ratio: 4.0
            qkv_bias: True
            drop_rate: 0.0
            attn_drop_rate: 0.0
            drop_path_rate: 0.0
            norm_layer: "nn.LayerNorm"
            patch_norm: False
            use_checkpoint: True
            spatial_dims: 3
            downsample: "merging"
            use_v2: False
            pretrained_ckpt: ""
            # pretrained_ckpt: "./checkpoints/official_ckpts/model_swinvit.pt" 

    vision_augment:
        max_drop: 0.3
        max_block_sz: 0.25
        tolr: 0.05

    vision_ssl_module:
        contrastive_bottleneck: "pooling" # choose from "pooling", "convolution", "self-attention"
        contrastive_hidden_dim: 512
        rotation_bottleneck: "pooling" # choose from "pooling", "convolution", "self-attention"
        reconstruction_bottleneck: "deconv" # choose from "deconv" or "unet"
        enable_contras: True
        enable_rot: True
        enable_recon: True

    vision_ssl_loss:
        contras_loss_temperature: 0.5
        learnable_temp: False
        rot_loss: "cross_entropy"
        recon_loss: "l1" # choose from "l1", "l2", "smooth_l1"
        alpha_contras: 1.0
        alpha_rot: 1.0
        alpha_recon: 1.0

    save_recon_interval: 1000


text_ssl:

    enable_txt_mlm: False
    
    text_model:
        # "emilyalsentzer/Bio_ClinicalBERT" or "microsoft/BiomedVLP-CXR-BERT-general"
        # pretrained_weights: "microsoft/BiomedVLP-CXR-BERT-general"
        name: "sent_bert" # or base_bert
        hidden_size: 768
        intermediate_size: 3072
        max_position_embeddings: 512
        layer_norm_eps: 1.0e-12
        hidden_dropout_prob: 0.1
        num_hidden_layers: 18
        add_cross_attention: False
        chunk_size_feed_forward: 256
        is_decoder: False
        num_attention_heads: 6
        attention_probs_dropout_prob: 0.1
        hidden_act: "gelu"
        output_attentions: False
        output_hidden_states: True
        use_return_dict: True
        use_cache: False
        mlm_probability: 0.15
        
        # pretrained_ckpt: ""
        # pretrained_ckpt: "./checkpoints/official_ckpts/bert_base_uncased.pth" 
        pretrained_ckpt: "./checkpoints/official_ckpts/biomedvlp-cxr-bert-general.bin"

        layer_aggregate_fn: "sum" # or "mean"
        report_aggregate_fn: "none" # "sum" or "mean"
        sentence_aggregate_fn: "none" # "sum" or "mean"
        word_aggregate_fn: "none" # "sum" or "mean"

        layer_for_txt_ssl: 12
        num_decoder_layers: 6


    tokenizer:
        base_type: "microsoft/BiomedVLP-CXR-BERT-general" # cls_token_id: 2; sep_token_id: 3; mask_token_id: 4;
        max_len_report: 512
        max_len_sentence: 200
        max_num_sentences: 50
        sentence_modeling: True